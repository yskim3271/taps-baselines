import os
import time
import shutil
import torch
import torch.utils
import torch.nn.functional as F
import torch.distributed as dist

from torch.utils.tensorboard import SummaryWriter
from pathlib import Path

from enhance import enhance
from evaluate import evaluate
from utils import bold, copy_state, pull_metric, swap_state, LogProgress
from criteria import CompositeLoss

class Solver(object):
    def __init__(
        self, 
        data, 
        model, 
        optim, 
        args, 
        logger, 
        rank=0,     
        world_size=1, 
        device=None
    ):
        # Dataloaders and samplers
        self.tr_loader = data['tr_loader']      # Training DataLoader
        self.va_loader = data['va_loader']      # Validation DataLoader
        self.tt_loader = data['tt_loader']      # Test DataLoader for checking result samples
        self.ev_loader = data['ev_loader']      # Evaluation DataLoader
        self.tr_sampler = data['tr_sampler']    # Distributed sampler for training
        
        self.model = model
        self.optim = optim
        self.logger = logger

        # Basic config
        self.device = device or torch.device(args.device)
        self.rank = rank
        self.world_size = world_size
        self.is_distributed = world_size > 1
        
        self.epochs = args.epochs
        self.loss = args.loss
        self.clip_grad_norm = args.clip_grad_norm
        
        self.loss = CompositeLoss(args.loss).to(self.device)
        
        self.eval_every = args.eval_every   # interval for evaluation
        self.eval_only = args.eval_only     # if True, only evaluate, no training
            
        # Checkpoint settings
        self.checkpoint = args.checkpoint
        
        self.continue_from = args.continue_from
        
        if self.checkpoint and self.rank == 0:
            self.checkpoint_file = Path(args.checkpoint_file)
            self.logger.info("Checkpoint will be saved to %s", self.checkpoint_file.resolve())

        self.writer = None
        self.best_state = None
        self.history = []
        self.log_dir = args.log_dir
        self.samples_dir = args.samples_dir
        self.num_prints = args.num_prints
        self.args = args
        
        # Initialize or resume (checkpoint loading)
        self._reset()

    def _serialize(self):
        """ Save checkpoint and best model (only rank=0).
        
            - We save both a 'checkpoint.th' file and a 'best.th' file.
            - The 'checkpoint.th' contains model state, optimizer state, and history.
            - The 'best.th' contains the best performing model state so far.
        """
        if self.rank != 0:  # save only on rank 0
            return
        
        # Create a package dict
        package = {}
        package['model'] = copy_state(self.model.module.state_dict() if self.is_distributed else self.model.state_dict())
        package['optimizer'] = self.optim.state_dict()
        package['history'] = self.history
        package['args'] = self.args
        
        # Write to a temporary file first
        tmp_path = str(self.checkpoint_file) + ".tmp"
        torch.save(package, tmp_path)
        os.rename(tmp_path, self.checkpoint_file)

        # Save the best model separately to 'best.th'
        best_path = Path("best.th")
        tmp_path = str(best_path) + ".tmp"
        torch.save(self.best_state, tmp_path)
        os.rename(tmp_path, best_path)

    def _reset(self):
        """Load checkpoint if 'continue_from' is specified, or create a fresh writer if not."""
        if self.continue_from is not None:
            if self.rank == 0:
                self.logger.info(f'Loading checkpoint model: {self.continue_from}')
                
                # Attempt to copy the 'tensorbd' directory (TensorBoard logs) if it exists
                src_tb_dir = os.path.join(self.continue_from, 'tensorbd')
                dst_tb_dir = self.log_dir
                
                if os.path.exists(src_tb_dir):
                    # If the previous tensorboard logs exist, we either copy them
                    # to the new log dir or skip if it already exists.
                    if not os.path.exists(dst_tb_dir):
                        shutil.copytree(src_tb_dir, dst_tb_dir)
                    else:
                        # If the new log dir already exists, just issue a warning and do not overwrite
                        self.logger.warning(f"TensorBoard log dir {dst_tb_dir} already exists. Skipping copy.")
                    # Initialize the SummaryWriter to continue logging in the (possibly copied) directory
                    self.writer = SummaryWriter(log_dir=dst_tb_dir)
            
            package = None  # Initialize package to None for non-rank 0 processes
            
            # Rank 0 loads the checkpoint file from disk
            if self.rank == 0:
                ckpt_path = os.path.join(self.continue_from, 'checkpoint.th')
                package = torch.load(ckpt_path, map_location='cpu')

            if self.is_distributed:
                # Wait until rank 0 finishes loading the checkpoint
                dist.barrier()
                
                # Broadcast the loaded checkpoint object to all ranks
                obj_list = [package]
                dist.broadcast_object_list(obj_list, src=0)
                package = obj_list[0] 

                # Extract model and optimizer state
                model_state = package['model']
                optim_state = package['optimizer']
                
                # Load states into the DDP-wrapped model and optimizer
                self.model.module.load_state_dict(model_state)
                self.optim.load_state_dict(optim_state)
            else:
                # In non-distributed (single GPU or CPU) mode, just load directly
                model_state = package['model']
                optim_state = package['optimizer']
                self.model.load_state_dict(model_state)
                self.optim.load_state_dict(optim_state)
            
            # Now attempt to load the best checkpoint if it exists
            best_path = os.path.join(self.continue_from, 'best.th')
            if os.path.exists(best_path):
                self.best_state = torch.load(best_path, 'cpu')
            else:
                # If best.th does not exist, create a fallback best_state from the current model
                self.best_state = {
                    'model': copy_state(
                        self.model.module.state_dict() if self.is_distributed 
                        else self.model.state_dict()
                    )
                }
            
            # If there's any historical training metrics in the checkpoint, restore them
            if 'history' in package:
                self.history = package['history']
        else:
            # If there's no checkpoint to resume from, just create a fresh SummaryWriter
            self.writer = SummaryWriter(log_dir=self.log_dir)

    def train(self):
        """Main training loop, including optional evaluation phases."""
        # If eval_only is True, and we have a test loader (tt_loader), do evaluation only
        if self.eval_only and self.tt_loader and self.rank == 0: 
            metric = evaluate(self.args, self.model, self.tt_loader)
            return

        # If there's a history from the checkpoint, replay metrics
        if self.history and self.rank == 0:  
            self.logger.info("Replaying metrics from previous run")
            for epoch, metrics in enumerate(self.history):
                info = " ".join(f"{k.capitalize()}={v:.5f}" for k, v in metrics.items())
                self.logger.info(f"Epoch {epoch + 1}: {info}")
        
        if self.rank == 0:
            self.logger.info(f"Training for {self.epochs} epochs")
        
        # Main epoch loop
        for epoch in range(len(self.history), self.epochs):
            # Switch to training mode
            self.model.train()
            
            # If using a distributed sampler, set its epoch to ensure different random ordering each epoch
            if self.tr_sampler is not None:
                self.tr_sampler.set_epoch(epoch)
            
            start = time.time()
            if self.rank == 0:
                self.logger.info('-' * 70)
                self.logger.info("Training...")
            
            # Run one epoch of training
            train_loss = self._run_one_epoch(epoch)
            
            if self.rank == 0:
                self.logger.info(
                    bold(f'Train Summary | End of Epoch {epoch + 1} | '
                         f'Time {time.time() - start:.2f}s | Train Loss {train_loss:.5f}'))
            
            # Optionally run validation if va_loader is present
            if self.va_loader:
                self.model.eval()
                
                if self.rank == 0:
                    self.logger.info('-' * 70)
                    self.logger.info('Validation...')
                
                with torch.no_grad():
                    valid_loss = self._run_one_epoch(epoch, valid=True)
                
                if self.rank == 0:
                    self.logger.info(
                        bold(f'Valid Summary | End of Epoch {epoch + 1} | '
                            f'Time {time.time() - start:.2f}s | Valid Loss {valid_loss:.5f}'))
            else:
                valid_loss = 0
            
            # If distributed, we can synchronize here so that next epoch starts together
            if self.is_distributed:
                dist.barrier()
                
            # rank=0 handles model saving and test set evaluation
            if self.rank == 0:
                best_loss = min(pull_metric(self.history, 'valid') + [valid_loss])
                metrics = {'train': train_loss, 'valid': valid_loss, 'best': best_loss}
                
                # Update best_state if we got a new best validation loss
                if valid_loss == best_loss:
                    self.logger.info(bold('New best valid loss %.4f'), valid_loss)
                    self.best_state = {'model': copy_state(self.model.module.state_dict() if self.is_distributed else self.model.state_dict())}

                # Evaluate on ev_loader (test set) every eval_every epochs (or last epoch)
                if self.eval_every is not None:
                    if ((epoch + 1) % self.eval_every == 0 or epoch == self.epochs - 1) and self.ev_loader:
                        self.logger.info('-' * 70)
                        self.logger.info('Evaluating on the test set...')
                        
                        # Temporarily swap model weights with best_state for evaluation
                        with swap_state(self.model.module if self.is_distributed else self.model, self.best_state['model']):
                            metric = evaluate(self.args, self.model, self.ev_loader, epoch, self.logger)
                            metrics.update(metric)
                            
                            # Log test metrics to TensorBoard
                            for k, v in metric.items():
                                self.writer.add_scalar(f"Test/{k.capitalize()}", v, epoch)

                            if self.tt_loader:
                                self.logger.info('Enhance and save samples...')
                                enhance(self.args, self.model, self.tt_loader, epoch, self.logger, self.samples_dir)
                
                # Append metrics to history and print summary
                self.history.append(metrics)
                info = " | ".join(f"{k.capitalize()} {v:.5f}" for k, v in metrics.items())
                self.logger.info('-' * 70)
                self.logger.info(bold(f"Overall Summary | Epoch {epoch + 1} | {info}"))
                
                # Checkpoint serialization
                if self.checkpoint:
                    self._serialize()
                    self.logger.debug("Checkpoint saved to %s", self.checkpoint_file.resolve())
                    

    def _run_one_epoch(self, epoch, valid=False):
        """Run a single epoch of training or validation.
        
        Args:
            epoch (int): The current epoch index.
            valid (bool): Whether this is a validation epoch.
        Returns:
            float: The average loss over the epoch.
        """
        total_loss = 0.0
        data_loader = self.tr_loader if not valid else self.va_loader
        
         # If the sampler has a set_epoch method, call it to shuffle data consistently across ranks
        if hasattr(data_loader, 'sampler') and hasattr(data_loader.sampler, 'set_epoch'):
            data_loader.sampler.set_epoch(epoch)

        label = ["Train", "Valid"][valid]
        name = label + f" | Epoch {epoch + 1}"
        
        # For rank=0, use a LogProgress, else just iterate the data
        if self.rank == 0:
            logprog = LogProgress(self.logger, data_loader, updates=self.num_prints, name=name)
        else:
            logprog = data_loader
        
        for i, data in enumerate(logprog):
            # Unpack data; if valid, there's an extra item (file id), else just 4 items
            if valid:
                tm, am, mask = data
            else:
                tm, am = data
                mask = None
                
            # Move inputs to the correct device
            if mask is not None:
                mask = mask.to(self.device)
                
            tm = tm.to(self.device)
            am = am.to(self.device)
            
            # Forward pass
            am_hat = self.model(tm)
            
            # Compute loss
            loss_all, loss_dict = self.loss(am_hat, am, mask)

            # For distributed training, we do all_reduce
            if self.is_distributed:
                dist.all_reduce(loss_all)
                loss_all = loss_all / self.world_size
            
            total_loss += loss_all.item()

            if not valid:
                # Training step
                self.optim.zero_grad()
                
                if self.rank == 0:
                    # Log current losses in the progress bar
                    for i, (key, value) in enumerate(loss_dict.items()):
                        if i == 0:
                            logprog.update(**{key.capitalize(): format(value, "4.5f")})
                        else:
                            logprog.append(**{key.capitalize(): format(value, "4.5f")})
                        self.writer.add_scalar(f"train/{key.capitalize()}", value, epoch * len(data_loader) + i)
                    self.writer.add_scalar("train/Loss", loss_all.item(), epoch * len(data_loader) + i)
                
                # Backpropagation
                loss_all.backward()
                
                # Gradient clipping
                if self.clip_grad_norm:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.clip_grad_norm)
                    
                # Optimizer step
                self.optim.step()
                
            else:
                # Validation step (rank=0 logs)
                if self.rank == 0:
                    for i, (key, value) in enumerate(loss_dict.items()):
                        if i == 0:
                            logprog.update(**{key.capitalize(): format(value, "4.5f")})
                        else:
                            logprog.append(**{key.capitalize(): format(value, "4.5f")})
                    self.writer.add_scalar("valid/Loss", loss_all.item(), epoch * len(data_loader) + i)

                    # Optionally log audio every 100 items
                    if self.eval_every:
                        if i % 100 == 0 and epoch % self.eval_every == 0:
                            if epoch == 0:
                                self.writer.add_audio('gt/y_{}'.format(i), am[0], epoch, self.args.sampling_rate)
                            self.writer.add_audio('enhanced/y_hat_{}'.format(i), am_hat[0], epoch, self.args.sampling_rate)
        
        # Return the average loss over the entire epoch
        return total_loss / len(data_loader)